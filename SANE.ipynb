{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QT4J9fJXbMk"
      },
      "source": [
        "# **Strategic (S) Autonomous (A) Noisy (N) Experiments (E)**\n",
        "Nov, 2024\n",
        "\n",
        "This is the notebook contains only the developed functions for SANE\n",
        "\n",
        "-- Notebook prepared by **Arpan Biswas**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages"
      ],
      "metadata": {
        "id": "YYNLqirYxKzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import os\n",
        "import h5py\n",
        "import sidpy\n",
        "import time\n",
        "\n",
        "from mycolorpy import colorlist as mcp\n",
        "\n",
        "gpax.utils.enable_x64()  # enable double precision\n",
        "\n",
        "from warnings import filterwarnings\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from atomai.utils import get_coord_grid, extract_patches_and_spectra, extract_subimages\n",
        "\n",
        "import h5py\n",
        "import sidpy\n",
        "import pyUSID as usid"
      ],
      "metadata": {
        "id": "xztr0YKV0IKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of functions for SANE module\n",
        "1. Human assessment (Initial data)\n",
        "2. prediction model for constraint\n",
        "3. Hybrid cost based acquisiton function\n",
        "4. Function to check for the region of interest\n",
        "5. Prediction model for function evaluation\n",
        "6. Plot function\n"
      ],
      "metadata": {
        "id": "ZvPIoiIMxUV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Function for perform voting to assess spectral quality\n",
        "def votes_initialdata(img, X_indices, data, idx):\n",
        "    beps_wv= data[0]\n",
        "    spectra = data[1]\n",
        "    idx_good = []\n",
        "    idx_bad = []\n",
        "    for i in range(0, len(idx)):\n",
        "        #plt.figure(figsize = (5, 4))\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), dpi=200)\n",
        "        ax1.imshow(img, origin = \"lower\")\n",
        "        ax1.scatter(X_indices[i, 0], X_indices[i, 1], color = 'red', marker = 'x', s =50)\n",
        "        #ax1.set_xlabel(\"Composition\")\n",
        "        #ax1.set_ylabel(\"y\")\n",
        "        #ax1.legend()\n",
        "        ax1.set_title(\"Initial target values, location\"+str(X_indices[i,:]))\n",
        "        ax2.plot(beps_wv, spectra[int(X_indices[i, 0]), int(X_indices[i, 1]), :], linewidth = 2)\n",
        "        ax2.set_title(\"Initial spectral\")\n",
        "        ax2.set_xlabel(\"Wavelength\")\n",
        "        ax2.set_ylabel(\"Polarization\")\n",
        "        plt.show()\n",
        "\n",
        "        # User votes the quality of the spectral- Domain knowledge injection in BO initialization\n",
        "        print(\"Is the current spectral good? y- Yes or press ENTER- No\")\n",
        "        time.sleep(0.5)\n",
        "        ip = input(\"Answer: \")\n",
        "        if (ip == 'Y' or ip == 'y'):\n",
        "            idx_good = np.hstack((idx_good, idx[i]))\n",
        "            #idx_good.append(idx[i])\n",
        "        else:\n",
        "            idx_bad = np.hstack((idx_bad, idx[i]))\n",
        "            #idx_bad.append(idx[i])\n",
        "    idx_good = idx_good.astype(int)\n",
        "    idx_bad = idx_bad.astype(int)\n",
        "\n",
        "    return idx_good, idx_bad\n",
        "\n",
        "# Function for perform GP-- to contrain between good and bad region\n",
        "def run_constraindkl (xtrain, ytrain, xtest, fn = None, fn_priors = None):\n",
        "    rng_key, rng_key_predict = gpax.utils.get_keys(1000)\n",
        "    data_dim = xtrain.shape[-1]\n",
        "    # update GP posterior\n",
        "    dkl = gpax.viDKL(data_dim, 2, kernel='RBF')\n",
        "    dkl.fit(  # you may decrease step size and increase number of steps (e.g. to 0.005 and 1000) for more stable performance\n",
        "        rng_key, xtrain, ytrain, num_steps=1000, step_size=0.005)\n",
        "\n",
        "    y_pred, y_var = dkl.predict(rng_key_predict, xtest)\n",
        "\n",
        "    return dkl, y_pred, y_var\n",
        "\n",
        "# Function for perform constrained cost based acq function\n",
        "def cost_acqfun(models, train_x, Data, cost_params, params, isnorm=False):\n",
        "    \"\"\"\n",
        "    cost based acquisition function- EI based\n",
        "    Args:\n",
        "        model: krigging model\n",
        "        train_x: training data\n",
        "        Data: Unexplored data to evaluate for next sample selection in list with actual values and normalized values\n",
        "        cost_params: Params required for cost aware acquisition function\n",
        "        params: Params required for standard EI acq function\n",
        "        ieval: All the current locations explored\n",
        "        isnorm: If the data need to be normalized. Default is False\n",
        "\n",
        "    Returns:\n",
        "        array of acquisition function values\n",
        "    \"\"\"\n",
        "\n",
        "    data_real = Data[0] # True data\n",
        "    data_norm = Data[1] #Normalize data\n",
        "    indices_real = Data[2] # indices data\n",
        "\n",
        "    best_x = cost_params[0]\n",
        "    exp_x = cost_params[1]\n",
        "    a = cost_params[2]\n",
        "    b = cost_params[3]\n",
        "    switch = cost_params[4]\n",
        "\n",
        "    if isnorm == False:\n",
        "        data = data_real\n",
        "        # print(data)\n",
        "    else:\n",
        "        data = data_norm\n",
        "        # print(data)\n",
        "\n",
        "    dkl_model = models\n",
        "    # Compute standard acq function\n",
        "    standard_acq = gpax.acquisition.EI(params, dkl_model, data, maximize=True, noiseless=True)\n",
        "    standard_acq = np.array(standard_acq)\n",
        "\n",
        "    # Check for stability\n",
        "    standard_acq[np.isnan(standard_acq)==True] = np.random.uniform(low=0.0, high=1.0, size=standard_acq[np.isnan(standard_acq)==True].shape)*1e-3\n",
        "    # Eliminate evaluated samples from consideration to avoid repeatation in future sampling\n",
        "    #standard_acq[ieval] = 0\n",
        "    # Compute cost on non-normalized data\n",
        "    prev_x = train_x[-1,:]\n",
        "    x_center = best_x\n",
        "    l_exp = len(exp_x)-1\n",
        "    cost = np.zeros((standard_acq.shape[0], 2))\n",
        "    c = np.zeros((standard_acq.shape[0]))\n",
        "\n",
        "    for i in range(indices_real.shape[0]):\n",
        "        cost[i, 0] = a*(np.absolute(indices_real[i,0]-x_center[0])+np.absolute(indices_real[i,1]-x_center[1])) # encourage local exploration\n",
        "\n",
        "        for j in range(l_exp): # encourage already invested locations of potential region of interest when global exploration is continued\n",
        "          cost[i, 1] = cost[i, 1] + (np.absolute(indices_real[i,0]-train_x[exp_x[j], 0])+np.absolute(indices_real[i,1]-train_x[exp_x[j], 1]))\n",
        "          cost[i, 1] = cost[i, 1]*b\n",
        "        if l_exp > 0:\n",
        "          cost[i, 1] = cost[i, 1]/l_exp\n",
        "          #cost[:, 1] = 0\n",
        "        c[i] = -cost[i, 0] + cost[i, 1]\n",
        "        #if c[i] < 0:\n",
        "        #  c[i] = 0.1\n",
        "\n",
        "    c_norm = (c-np.min(c))/(np.max(c)-np.min(c))\n",
        "\n",
        "    if (np.max(standard_acq)+1e-5)/((np.sum(standard_acq))+1e-5) >= 0.75:\n",
        "        print(\"Full local exploration initiated\")\n",
        "        #standard_acq = 0*standard_acq\n",
        "        cost_acq = c_norm # discourage already invested locations of potential region of interest when global learning is completed\n",
        "\n",
        "    else:\n",
        "        if switch == 1: #Explorative\n",
        "          cost_acq = standard_acq*(np.sum(cost, axis=1)/np.max(np.sum(cost, axis=1)))\n",
        "        else: #Exploitative\n",
        "          cost_acq = standard_acq/(np.sum(cost, axis=1)/np.max(np.sum(cost, axis=1)))\n",
        "        #c=-c+1\n",
        "        #cost_acq = standard_acq/c\n",
        "        #standard_acq = (standard_acq - np.min(standard_acq))/(np.max(standard_acq)- np.min(standard_acq))\n",
        "        #c= 1-c\n",
        "        #cost_acq = standard_acq*c*100\n",
        "\n",
        "    return cost_acq\n",
        "\n",
        "# Function to check region of interest/given the region is a good region\n",
        "def check_ROI(train_x, train_y, best_x, best_y, exp_x):\n",
        "    #best_y2 = np.max(train_y[exp_x[-1]+1:])\n",
        "    #best_x2 = train_x[np.argmax(train_y[exp_x[-1]+1:])]\n",
        "    #Find Euclidean distance between best x and the later explored x\n",
        "\n",
        "    l = len(train_x[exp_x[-1]+1:])\n",
        "    l_exp = len(exp_x)-1\n",
        "    dist_x = np.zeros(l)\n",
        "    dist_y = np.zeros(l)\n",
        "    dist_z = np.zeros(l)\n",
        "    for i in range(l):\n",
        "        dist_x[i] = np.absolute(best_x[0] - train_x[exp_x[-1]+i+1,0]) + np.absolute(best_x[1] - train_x[exp_x[-1]+i+1,1])\n",
        "        dist_y[i] = train_y[exp_x[-1]+i+1]/best_y\n",
        "        for j in range(l_exp): # discourage already invested locations of potential region of interest\n",
        "          dist_z[i] = dist_z[i] + np.absolute(train_x[exp_x[-1]+i+1,0] - train_x[exp_x[j],0]) + np.absolute(train_x[exp_x[-1]+i+1,1] - train_x[exp_x[j],1])\n",
        "        dist_z[i] = dist_z[i]/l_exp\n",
        "\n",
        "    dim_x = len(best_x)\n",
        "    #print(dim_x)\n",
        "\n",
        "    F = (dist_x * dist_y * dist_z)/dim_x\n",
        "    F_max = np.max(F)\n",
        "    F_imax= np.argmax(F)\n",
        "\n",
        "\n",
        "    prob_ROI = (dist_x[F_imax] + dist_y[F_imax])/dim_x\n",
        "    if prob_ROI > 1:\n",
        "        prob_ROI = 1\n",
        "    #print(F_max, prob_ROI)\n",
        "    #D = np.max(dist_x/dim_x) #Measure closeness\n",
        "\n",
        "    #O = np.max(dist_y) #Measure optimality\n",
        "    # We want to maximize D and O\n",
        "    #print(D, O)\n",
        "    #prob_ROI = min(D, O)\n",
        "    #print(prob_ROI)\n",
        "    #Probabilistic choice of identify the next best solution as a potential ROI\n",
        "    #np.random.seed(1000)\n",
        "    if np.random.choice(np.arange(2), size=1, p =[1-prob_ROI, prob_ROI]) == 1: #New ROI\n",
        "        exp_x = np.hstack((exp_x, int(exp_x[-1]+F_imax+1)))\n",
        "        print(\"New potential region of interest found with prob=\", prob_ROI)\n",
        "    else:\n",
        "        exp_x = exp_x #No new ROI\n",
        "        print(\"No region of interest found\")\n",
        "\n",
        "\n",
        "    return exp_x\n",
        "\n",
        "\n",
        "# Function for performing a GP step, here we use structure GP (sGP)\n",
        "def run_statdKL (xtrain, ytrain, xindices, Data, fn = None, fn_priors = None, constraint = None, pen = 1, cost_params = None, isnorm=False):\n",
        "  data_real = Data[0] # True data\n",
        "  data_norm = Data[1] #Normalize data\n",
        "\n",
        "\n",
        "  rng_key, rng_key_predict = gpax.utils.get_keys(1000)\n",
        "\n",
        "  data_dim = xtrain.shape[-1]\n",
        "  # update posterior\n",
        "  dkl = gpax.viDKL(data_dim, 2, kernel='Matern')\n",
        "  dkl.fit(  # you may decrease step size and increase number of steps (e.g. to 0.005 and 1000) for more stable performance\n",
        "      rng_key, xtrain, ytrain, num_steps=1000, step_size=0.005)\n",
        "\n",
        "  # Compute acquisition function\n",
        "  #obj = gpax.acquisition.UCB(rng_key_predict, gp_model, X_test)\n",
        "  cost_acq = cost_acqfun(dkl, xindices, Data, cost_params, rng_key_predict)\n",
        "\n",
        "  if constraint is not None:\n",
        "    cpen = np.ceil(np.max(cost_acq))\n",
        "    #print(cpen, np.max(cost_acq), np.max(constraint), np.min(constraint))\n",
        "    for i in range(0, len(constraint)):\n",
        "      if constraint[i] <= 0:\n",
        "        cost_acq[i] = cost_acq[i] + (pen*cpen*constraint[i])\n",
        "\n",
        "  # Get GP prediction\n",
        "  if isnorm == False:\n",
        "    y_pred, y_var = dkl.predict(rng_key_predict, data_real)\n",
        "\n",
        "  else:\n",
        "    y_pred, y_var = dkl.predict(rng_key_predict, data_norm)\n",
        "\n",
        "\n",
        "  return dkl, cost_acq, y_pred, y_var\n",
        "\n",
        "\n",
        "def plot_result(indices, obj, data):\n",
        "    beps_wv= data[0]\n",
        "    spectra = data[1]\n",
        "    for i in range(0,len(obj)):\n",
        "      if obj[i] < 0:\n",
        "        obj[i] = 0\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    a = plt.scatter(indices[:, 0], indices[:, 1], s=10, c=obj)\n",
        "    plt.colorbar(a)\n",
        "    next_point = indices[obj.argmax()]\n",
        "    plt.scatter(next_point[0], next_point[1], s=10, marker='s', c='r', linewidths=2)\n",
        "    #plt.xticks([])\n",
        "    #plt.yticks([])\n",
        "    plt.title(\"Acquisition Values\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.plot(beps_wv, spectra[int(next_point[0]), int(next_point[1]), :], linewidth = 2)\n",
        "    plt.title(\"spectral at location:\" +str(next_point))\n",
        "    plt.xlabel(\"Wavelength\")\n",
        "    plt.ylabel(\"Polarization\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7KTlXXeFx6zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script for human assesment to contraint metric"
      ],
      "metadata": {
        "id": "43Hrud_9zMwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_vote = np.zeros(len(y_measured))\n",
        "for i in range(0, len(y_vote)):\n",
        "    y_good = 0\n",
        "    y_bad = 0\n",
        "    for j1 in range(0, len(idx_good)):\n",
        "      y_good = y_good + np.absolute(indices_measured[i, 0] -  indices_all[idx_good[j1], 0]) + np.absolute(indices_measured[i, 1] -  indices_all[idx_good[j1], 1])\n",
        "    y_good = y_good/len(idx_good)\n",
        "    #print(y_good)\n",
        "\n",
        "    for j2 in range(0, len(idx_bad)):\n",
        "      y_bad = y_bad + np.absolute(indices_measured[i, 0] -  indices_all[idx_bad[j2], 0]) + np.absolute(indices_measured[i, 1] -  indices_all[idx_bad[j2], 1])\n",
        "    y_bad = y_bad/len(idx_bad)\n",
        "    #print(y_bad)\n",
        "\n",
        "    y_vote[i] = y_bad - y_good # Turning into maximization problem\n",
        "\n",
        "\n",
        "print(idx_good, idx_bad, y_vote)"
      ],
      "metadata": {
        "id": "3C-qeOskzROI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SANE parameter settings"
      ],
      "metadata": {
        "id": "FT8JTfIJziAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Swtiching parameter\n",
        "k = np.zeros(100)\n",
        "k[60:] = k[60:] + 1\n",
        "plt.plot(k)\n",
        "\n",
        "# Below parameters are currently fixed as 1\n",
        "c_loc = 1\n",
        "c_glo = 1"
      ],
      "metadata": {
        "id": "9hfWP4i9zkSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full workflow is provided in the Analysis folder for each test cases"
      ],
      "metadata": {
        "id": "jY6RiAp2z8dK"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOrhFQKEqw4iq3LPp0mMThy"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}